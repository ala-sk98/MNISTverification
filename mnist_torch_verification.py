# -*- coding: utf-8 -*-
"""mnist_torch_verification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VD7HN3BN-Cn-ssZ2YiaJlIRPCZ_rtyru
"""

!pip install kymatio
!pip install torchmetrics
# !pip install wandb
# !wandb login

import torch
import torchvision
import torchmetrics
from torch import nn
import numpy as np

from kymatio.torch import Scattering2D
import matplotlib.pyplot as plot 
# import wandb

# wandb.init(project="stepscan-project", entity="ala_sk")

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

from torchvision import datasets
from torchvision.transforms import ToTensor
train_data = datasets.MNIST(
    root = 'data',
    train = True,                         
    transform = ToTensor(), 
    download = True,            
)
test_data = datasets.MNIST(
    root = 'data', 
    train = False, 
    transform = ToTensor()
)

print(train_data)
print(test_data)
print(train_data.data.size())
print(train_data.targets.size())
print(train_data.targets[100],train_data.targets[50])

train_data_backup = np.copy(train_data.targets)
test_data_backup = np.copy(test_data.targets)

from torch.utils.data import DataLoader
from torch.utils.data.sampler import WeightedRandomSampler, SubsetRandomSampler

def data_loaders(train_data, test_data):

    dataset_size = len(train_data)
    dataset_indices = list(range(dataset_size))
    np.random.shuffle(dataset_indices)
    val_split_index = int(np.floor(0.9 * dataset_size))
    train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]
    train_sampler = SubsetRandomSampler(train_idx)
    val_sampler = SubsetRandomSampler(val_idx)


    class_count = [9000, 1000]
    class_weights = 1./torch.tensor(class_count, dtype=torch.float) 
    target_list = torch.tensor(train_data.targets)
    class_weights_all = class_weights[target_list]
    num_samples = 40
    weighted_sampler_train = WeightedRandomSampler(weights=class_weights_all,
                                            num_samples=num_samples,
                                            replacement=True
                                            )

    class_count = [9000, 1000]
    class_weights = 1./torch.tensor(class_count, dtype=torch.float) 
    target_list = torch.tensor(test_data.targets)
    class_weights_all = class_weights[target_list]
    num_samples = 20
    weighted_sampler_test = WeightedRandomSampler(weights=class_weights_all,
                                            num_samples=num_samples,
                                            replacement=True
                                            )
    loaders = {
        'train' : torch.utils.data.DataLoader(train_data,                                           
                                              batch_size=20, 
                                              shuffle=False,
                                              num_workers=1,
                                              sampler=weighted_sampler_train
                                              ), 
        
        'test'  : torch.utils.data.DataLoader(test_data, 
                                              batch_size=20, 
                                              shuffle=False, 
                                              num_workers=1,
                                              sampler=weighted_sampler_test
                                              )
    }
    
    return loaders

input_size = 9*14*14 
hidden_layer = 500 
classes = 10
class Model(torch.nn.Module):
    def __init__(self):
         super(Model, self).__init__()
         self.input_size = input_size
         self.l1 = nn.Linear(input_size, hidden_layer) 
         self.relu = nn.ReLU()
         self.l2 = nn.Linear(hidden_layer, classes) 
    def forward(self, x):
         x = self.l1(x)
         x = self.relu(x)
         output = self.l2(x)
         return output


model = Model()

print('The model:')
print(model)

from torch import optim
from torch import nn
loss_func = nn.CrossEntropyLoss()   
loss_func
optimizer = optim.Adam(model.parameters(), lr = 0.00001)   
optimizer

from torch.autograd import Variable
from torchmetrics import Accuracy
import pandas as pd
import seaborn as sn
import numpy as np

# ID = np.linspace(0, 9, 10)
# print(ID)
ID = np.unique(train_data_backup)
scores_list = np.empty((10,1))

for user in ID:
    train_data.targets = np.copy(train_data_backup)
    test_data.targets = np.copy(test_data_backup)

    # print(np.unique(train_data.targets))
    # print(train_data.targets[train_data.targets==user])
    train_data.targets[train_data.targets!=user] = -1
    train_data.targets[train_data.targets==user] = 1
    train_data.targets[train_data.targets==-1] = 0

    # print(np.unique(train_data.targets))
    test_data.targets[test_data.targets!=user] = -1
    test_data.targets[test_data.targets==user] = 1
    test_data.targets[test_data.targets==-1] = 0

    # print(np.unique(test_data.targets))

    loaders = data_loaders(train_data, test_data)
    num_epochs = 10
    accuracy = Accuracy()
    scattering = Scattering2D(J=1, shape=(28, 28))

    def train(num_epochs, cnn, loaders):

        cnn.train()
        # print(enumerate(loaders['train']))   
        # Train the model
        total_step = len(loaders['train'])
        for epoch in range(num_epochs):
            for i, (images, labels) in enumerate(loaders['train']):
                images = scattering(images)
                b_x = images.reshape(-1, 9*14*14)
                b_y = Variable(labels)
                output = cnn(b_x)

                # gives batch data, normalize x when iterate train_loader
                # b_x = Variable(images)   # batch x
                # b_y = Variable(labels)   # batch y
                # output = cnn(b_x)[0]               
                loss = loss_func(output, b_y)
                acc = 100 * accuracy(output, b_y)
                # clear gradients for this training step   
                optimizer.zero_grad()           

                # backpropagation, compute gradients 
                loss.backward()    
                # apply gradients             
                optimizer.step()                
                print (f'Epochs [{epoch + 1}/{num_epochs}], Step[{i + 1}/{total_step}], Losses: {loss.item():.4f}, Accuracy: {acc.item():.2f} %')

                # if (i+1) % 100 == 0:
                #     print (f'Epochs [{epoch + 1}/{num_epochs}], Step[{i + 1}/{total_step}], Losses: {loss.item():.4f}, Accuracy: {acc.item():.2f} %')

                #     # print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f} %' 
                #     #        .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), accuracy))
                #     pass
                # wandb.log({"train_accuracy": acc})
                # wandb.log({"train_loss": loss})
            pass
        # wandb.watch(cnn)
        return acc, cnn
    
    def test():
        # Test the model
        model.eval()
        with torch.no_grad():
            correct = 0
            total = 0
            for images, labels in loaders['test']:
                images = scattering(images)
                images = images.reshape(20, 9*14*14)
                test_output = model(images)
                pred_y = torch.max(test_output, 1)[1].data.squeeze()
                accuracy = (pred_y == labels).sum().item() / float(labels.size(0))
                pass
            print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)
            confmat = torchmetrics.ConfusionMatrix(num_classes=2)
            print(labels)
            print(pred_y)
            array = confmat(pred_y, labels)
            return array, accuracy
        pass

    print(f'train dataset for the user {user}:')
    score, network = train(num_epochs, model, loaders)
    torch.save(network, f'user_{user}.pt')
    array, acc = test()
    score = acc
    scores_list[user] = score

    print(score, scores_list)
    sn.heatmap(array, annot=True)
    plot.title(f"confusion matrix for user {user}")
    plot.show()


scores_array = np.array(scores_list)    
ones_array = np.ones((10,1))
data_array = np.concatenate((ones_array, ID.reshape(10,1), scores_array.reshape(10,1)), axis=1)
df = pd.DataFrame(data_array, columns = ['Experiment 1', 'ID', 'Test accuracy'])
sn.set_theme(style="whitegrid")
ax = sn.boxplot(x='Experiment 1', y='Test accuracy', data=df, width=0.5, showmeans=True)
ax = sn.swarmplot(x='Experiment 1', y='Test accuracy', data=df, color=".2", alpha=0.6)
plot.title("Box Plot")

plot.tight_layout()
plot.show()

print (scores_list)

def test():
    # Test the model
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in loaders['test']:
            images = scattering(images)
            images = images.reshape(20, 9*14*14)
            test_output = model(images)
            pred_y = torch.max(test_output, 1)[1].data.squeeze()
            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))
            pass
        print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)
        confmat = torchmetrics.ConfusionMatrix(num_classes=2)
        array = confmat(pred_y, labels)
        return(array)
    pass
test()

import pandas as pd
import seaborn as sn
array = test()
print(array)
sn.heatmap(array, annot=True)

plot.show()

sample = next(iter(loaders['test']))
imgs, lbls = sample

actual_number = lbls[:10].numpy()
actual_number

images = scattering(imgs[:10])
images = images.reshape(-1, 9*14*14)

test_output = model(images)
pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()
print(f'Prediction number: {pred_y}')
print(f'Actual number: {actual_number}')